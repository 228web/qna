% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{algorithm,caption,algpseudocode}
\usepackage{xcolor}

\newcommand\alert[1]{\textcolor{red}{#1}}
\newcommand\note[1]{\textcolor{blue}{#1}}
\newcommand\jb[1]{\textcolor{green!50!black}{#1}}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{ASSESS}{2014 New York City, USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Item Response Theory VS Q-matrices for Adaptive Testing}
%\subtitle{English subtitles
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{5} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor Jill-Jênn Vie\\
       \affaddr{ENS Cachan -- Bât. Cournot}\\
       \affaddr{61 av. du Président Wilson}\\
       \affaddr{94235 Cachan, France}\\
       \email{vie@jill-jenn.net}
% 2nd. author
\alignauthor
Fabrice Popineau\\
       \affaddr{Supélec -- Dép. informatique}\\
       \affaddr{3 rue Joliot Curie}\\
       \affaddr{91192 Gif-sur-Yvette, France}\\
       \email{fabrice.popineau@supelec.fr}
% 3rd. author
\alignauthor
Jean-Bastien Grill\\
       \affaddr{École normale supérieure}\\
       \affaddr{45 rue d'Ulm}\\
       \affaddr{75005 Paris}\\
       \email{grill@clipper.ens.fr}
\and
% 4th. author
\alignauthor Éric Bruillard\\
       \affaddr{ENS Cachan -- Bât. Cournot}\\
       \affaddr{61 av. du Président Wilson}\\
       \affaddr{94235 Cachan, France}\\
       \email{eric.bruillard@ens-cachan.fr}
% 5th. author
\alignauthor
Yolaine Bourda\\
       \affaddr{Supélec -- Dép. informatique}\\
       \affaddr{3 rue Joliot Curie}\\
       \affaddr{91192 Gif-sur-Yvette, France}\\
       \email{yolaine.bourda@supelec.fr}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
% \additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
% email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
% (The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{May 26, 2014}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Computerized Adaptive Testing (CAT) is a mode of testing which has gained increasing popularity over the past years (economic purposes): it selects the questions asked to the examinee in order to value her level very effectively, by using her answers to the previous questions.
Traditionally, CAT systems have been relying on Item Response Theory (IRT) in order to provide an effective measure of a latent ability in large-scale assessments.
In recent work in the field of psychometrics, cognitive diagnosis models have emerged, incorporating examinee skills in order to provide useful feedback. Those have been extensively studied in the literature and mostly rely on q-matrices.
In this paper, we propose a new framework for evaluating adaptive testing algorithms and use it to compare both the latent trait model using IRT and the cognitive diagnosis model using q-matrices.
We found that q-matrices perform better on both simulated data and real data.
\end{abstract}

% A category with the (minimum) three required fields
\category{}{Student assessment}{Adaptive testing}
%A category including the fourth, optional field follows...
\category{}{Active learning}{Generalized binary search}

\terms{Adaptive tests, q-matrices}

\keywords{Adaptive assessment}

\section{Introduction}
Automated assessment has been developed with the help of online initiatives such as the GMAT~\cite{Machin}, or more recently with the introduction of massive online open courses (MOOCs). The fact of ranking thousands of students automatically in evaluating or recruiting purposes or providing personal feedback to thousand of students automatically in formative purposes has been of growing interest.

If we already have for a certain test a large-scale database of item responses, it is natural to wonder which questions bring the most information about an examinee, i.e. if there is a ``best'' sequence according to which the questions should be asked. In oral examinations, the examiner usually picks the next question to ask according to the past performance of the examinee. Computerized adaptive testing can be seen as an automated version of this process: keep asking the best possible questions until enough information has been gathered. Indeed, if the examinee behaves typically, a subset of carefully chosen results may be enough to guess how she will perform on the rest of the questions.

In order to address this purpose, Item Response Theory (IRT) is of great help. Although IRT was first designed as a tool to detect~\cite{Snif} poor quality items in tests by establishing a link between difficulty of items and ability of contestants, it gained interest for computerized adaptive testing and several methods were proposed and implemented. According to her performance, an examinee gets a score. The simplicity of the IRT models allowed a strong theoretical analysis~\cite{Baker2004} while increasing its popularity.

More recently, the No Child Left Behind Act of 2001 called for more formative assessments, in order to detect early students with cognitive disabilities and urged the need of cognitive diagnosis models. Instead of a summative score, examinees would receive a detailed feedback, specifying which skills are mastered and which ones are not~\cite{Cheng2009}. These models allow researchers to reveal possible misconceptions of the examinee in order to suggest her appropriate exercises. Most of these models rely on an q-matrix specifying for each question the different skills required to solve it. For example, a fraction test may require 1) converting a whole number to a fraction, 2) separating a whole number from a fraction, 3) simplifying before subtracting, and so on~\cite{DeLaTorreDouglas2004}. Recently, adaptive tests have been designed in order to guess the skills of the examinee effectively~\cite{Huebner2010}.

Whereas . At first, q-matrices were specified by experts, but recent work in the educational data mining field tend to infer such q-matrices directly from student data, using hill-climbing techniques~\cite{Barnes2005}, non-negative matrix factorization~\cite{Desmarais2011} or  the EM algorithm~\cite{Huebner2010}. The skills extracted by data mining algorithms are therefore implicit, but have proven to produce better inference~\cite{Barnes2003}.

\subsection{Our contribution}

In this paper, we propose a new framework for evaluating adaptive testing algorithms and use it to compare the performances of both a traditional computerized adaptive testing using item response theory and a more recent cognitive diagnosis model from the literature in psychometrics using q-matrices. More precisely, we tend to answer the following question: given a budget of $B$ questions asked according to a certain adaptive selection rule, which model performs the best in predicting the answers of the examinee over the remaining questions?

One could wonder why we compare a model designed for assessment versus a model designed for feedback. Although q-matrix were first designed as a cognitive diagnostic model to provide a feedback, it is also possible to use them in the context of large-scale assessments. Indeed, we can estimate the performance of an examinee, according to her computed skills. Thus, the framework we propose for evaluation captures both models.

Whereas IRT is the most commonly used model for adaptive testing, Q-matrix turns out to be a better model for fine-grained cognitive diagnosis. In fact, our results show the cognitive diagnosis model outperforms IRT on both simulated and real data.

\subsection{Outline}

We will first present the computerized adaptive testing framework and two popular models chosen respectively from item response theory and cognitive diagnosis. We then detail the algorithms chosen in the two corresponding adaptive tests implementations and the proposed framework for evaluating them in our simulation. Finally, we will then present our results on both simulated and real data.

\section{Background and Related Work}

We present here two models that allow us to predict performance of the examinee.

\subsection{Computerized adaptive testing}

In a computerized adaptive test (CAT), the examinee is presented with adaptively selected questions according to her previous performance. Thus, a CAT framework relies on two main subroutines:
\begin{itemize}
\item \textsc{NextItem} : the item selection algorithm, that will pick the next question to ask according to the previous answers of the examinee;
\item \textsc{TerminationRule} : a condition that will end the test, when enough information has been gathered and the competence has been measured satisfyingly.
\end{itemize}

Common criteria for the item selection rule are minimizing the Shannon entropy of the distribution on the parameters, or maximizing the Kullback-Leibler divergence. % Plus précis ?

The framework of a CAT can be represented by the following algorithm.

\subsection{Item Response Theory}

For our needs, IRT features a commonly used model allowing to compute the probability item response. Basically, IRT estimate the ability of a student by a unique real number $\theta$ and characterize each question by two real numbers:

\begin{itemize}
\item the \emph{difficulty} $d$, corresponding to the required ability for answering the question correctly; % FIX-MOI ÇA STP TMTC
\item the \emph{discrimination} $\delta$, representing the capacity of the question to distinguish between students that have the required ability from the other ones.
\end{itemize}

Knowing the ability $\theta$ of a given student and both the difficulty $d$ and the discrimination parameter $\delta$ of a given question, the model allows us to compute the probability of this student correctly answering this question:
\[ \Pr\{success\} = \frac1{1+e^{\delta(\theta - d)}} \]

The challenge is to determine the parameters that fit the most the student data.

\alert{Évaluer des étudiants efficacement et le choix principal de modèle pour computerized adaptive testing}

\alert{Related work à propos des selection criteria}

A key algorithm lies within the item selection process, we want to select the best question according to a certain criteria, that will allow us to narrow at best the confidence interval around $\theta$.

\alert{IRT CAT}
\alert{Item selection via formule de Bayes}

\subsection{Cognitive Diagnosis Model}

Let us assume there are $K$ different skills. A user can be modeled by a binary vector $(a_1, \ldots, a_K$, called \emph{state}, representing her master of the different skills. 

\alert{Liens entre questions et compétences item-to-skill [Desmarais]}

A $q$-matrix \cite{Tatsuoka1983} represents, for each question, the different skills required to answer the question. $Q_{ij}$ is equal to 1 if the skill $j$ is required to solve the question $i$, 0 otherwise. We consider here the DINA model~\cite{Desmarais2012}, where a student will answer correctly a question if and only if she has all the required skills. For example, in the following q-matrix, skills 1 and 2 are required to answer the first question, skills 1 and 3 for the second question and only skill 3 for last question. % Il y a aussi d'autres trucs
\[ Q = \left(\begin{array}{lll}
1 & 1 & 0\\
1 & 0 & 1\\
0 & 0 & 1
\end{array}\right) \]

\alert{Il existe d'autres approches : il faut un seul skill parmi ceux demandés pour répondre correctement, plus des skills sont maitrisés plus la probabilité de répondre correctement est élevée, certains considèrent des états d'apprentissage intermédiaires (0, 1/2, 1) ou l'ajout de paramètres slip and guessing. }

\alert{Pour chaque user, il y a $2^K$ états possibles. On maintient pour chacun une distribution de probabilité sur les états (donc $2^K$ réels) ~\cite{Huebner2010}.}

\alert{Connaissant une distribution de probabilité, il est facile d'estimer la probabilité de répondre aux questions. L'update de l'état est fait via la formule de Bayes}

\alert{Formule de Bayes}

\alert{Exemples avec $K = 3$}

\alert{Lorsque des experts fixent cette q-matrice, on peut donner un feedback explicite, mais ici on s'intéresse seulement à prédire les résultats. On peut donc se libérer de cette contrainte de q-matrice imposée et en calculer une qui colle au mieux aux données}

\alert{Néanmoins : determine q-matrix from student data is currently an open field of research}

\section{Our Test Framework}

Our student data is a binary matrix contestants $\times$ questions where $c_{ij}$ is 1 if contestant $i$ answered the question $j$ correctly, 0 otherwise. We used a technique similar to cross-validation to compare those scenarios: given a training set of all students but one, we are trying to predict the performance of that one after having asked her a certain budget $B$ of questions. Thus, our \textsc{TerminationRule} will be : ``There have been $B$ questions asked so far.''

\begin{algorithm}
\caption*{\textbf{Computerized Adaptive Testing Framework}}
\begin{algorithmic}
\Procedure{Test}{}
\State \Call{TrainingStep}{}
\State $i \gets 0$
\While{\textsc{TerminationRule} is not satisfied}
	\State $q_{i + 1} \gets \Call{NextItem}{q_1, r_1, \ldots, q_i, r_i, parameters}$
	\State Ask question $q_{i + 1}$ to the examinee
	\State Get reply $r_{i + 1}$
	\State \Call{Update}{} $parameters$ accordingly
\EndWhile
\State \Call{Evaluate}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{itemize}
\item Step 1. Train the model with a subset of student data.
\item Step 2. Choose next item to ask.
\item Step 3. Estimate parameters according to the answer.
\item Step 4. Compute probability and error.
\end{itemize}

\note{Le souci, c'est que lorsque $b$ tend vers le nombre de questions total, la performance devrait être complètement prédite ; dans le cas de IRT, ce n'est actuellement pas le cas.}

Steps 2 and 3 are repeated $b$ times, where $b$ represents the \emph{budget} of questions to ask.

\subsection{Item Response Theory Design}

\subsubsection{Test}
We used the R package catR~\cite{MagisRaiche12} for this part of our experiments.

For our first step, we determine from the training set all the parameters of the model, i.e. difficulty $d_j$ and discrimination $\delta_j$ of each question and ability $\theta_i$ of each training student.

We are then going to simulate a CAT for our examinee, estimating her ability $\theta$, initialized at 0. As an item selection rule, we choose the question realizing the MEPV (Minimum Expected Posterior Variance) over $\theta$.

For our third step, we estimate $\theta$ via Bayesian updates according to her answer.

For our fourth step, we compute her probability of answering correctly to the remaining questions in the bank and error.

\subsection{Cognitive Diagnosis Model Design}

For our first step, we determine the q-matrix $Q$, the slipping and guessing parameters according to our training set.

We are then going to simulate a CAT for our examinee, maintaining a probability distribution $\pi$ over the possible states, initialized at the uniform distribution. As an item selection rule, we choose the question minimizing the Shannon entropy of the distribution over the possible states. Please note that this is the same logic that the MEPV criterion used in the IRT design~\cite{Cheng2009}.

For our third step, we update $\pi$ via the Bayes rule according to her answer.

For our fourth step, we compute her probability of answering correctly to the remaining questions in the bank and error.

\subsection{Our Evaluation Framework}

We choose the log-likelihood to compare both performances.

\section{Results}

\subsection{Simulated data}

Here we first generate a random q-matrix and guessing and slipping parameters, then we determine the student data accordingly. Therefore our q-matrix is perfectly calibrated.

\subsection{True data}

LSAT.

\alert{Peut-être mettre plus que deux algorithmes (IRT et q-matrix) sur le graphe ?}

\section{Discussion and Future Work}

Add a last column to q-matrix

No study has been done yet over the number of skills

\section{Conclusions}

Outperforms on both simulated data and real data.

%\section{Acknowledgments}
%
%We wish to thank JBG for his valuable comments.

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
%\appendix
%Appendix A
%\section{If I have something more to say}

%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
